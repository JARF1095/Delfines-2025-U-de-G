# -*- coding: utf-8 -*-
"""Breast Cancer XG-Boost

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/135aAWTG4llhQthH9xJ1xVqFEcCV94tGz

# 1. Cargando Dataset y librerías
"""

import sklearn
assert sklearn.__version__>='0.20'

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings(action='ignore', message='^internal gelsd')

df = pd.read_csv('data.csv')


# Conteo de valores de la variable objetivo
df['diagnosis'].value_counts()

# B: Benigno, M: Maligno

# Realizando el cambio B->1, M->0
df['diagnosis'] = df['diagnosis'].replace({'M':0, 'B':1})

# Validando el cambio
df['diagnosis'].value_counts()

# Porcentaje de tumores benignos
df.diagnosis.mean()

# Identificación de variables
features = list(set(df.columns.tolist())-set(['Unnamed: 32', 'diagnosis', 'id']))
features

# Determinando matrices de datos
X = df[features]
y = df.diagnosis # Variable objetivo

"""# 2. Muestreo"""

# Muestreo de la data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify = y,
                                                    train_size = 0.6,
                                                    random_state = 123)

# Partiendo el 40% restante entre test y watch
X_watch, X_test, y_watch, y_test = train_test_split(X_test, y_test,
                                                    stratify = y_test,
                                                    train_size = 0.5,
                                                    random_state = 123)



"""## XgBoost"""

import numpy as np
import xgboost as XGBClassifier

# Para realizar el modelo de XGBoost necesitamos cambiar el formato de pd as DMatrix

# Cambiando formato a .DMatrix()
dtrain = XGBClassifier.DMatrix(X_train, label = y_train)
dwatch = XGBClassifier.DMatrix(X_watch, label = y_watch)
dtest = XGBClassifier.DMatrix(X_test, label = y_test)

# Definiendo parámetros (Aqui deben cambiar los alumnos)
# param ={'objective': 'binary:logistic', # Tipo de aprendizaje
#           'max_depth': 2, # Profundidad del árbol
#           'learning_rate': 0.15,
#           'min_data_leaf': 0.05, # Min data en cada hoja
#           'grow_policy': 'lossguide', #Aprendizaje por error
#           'eval_metric': 'error' # Para evaluar el modelo
#           #'seed': 123
#           }

param = {
    # Hiperparámetros especificados en la tabla
    'max_depth': 6,                  # Maximum depth d of each tree (default: 6)
    'learning_rate': 0.3,            # eta: Weight of each new tree (default: 0.3)
    'n_estimators': 100,             # K: Total number of trees (default: 100)
    'gamma': 0,                      # gamma: Minimum loss reduction to make split (default: 0)
    'min_child_weight': 1,           # w_min: Minimum sum of instance weight needed in child (default: 1)
    'subsample': 1.0,                # Fraction of data randomly sampled for each tree (default: 1.0)
    'colsample_bytree': 1.0,         # Fraction of features randomly sampled for each tree (default: 1.0)
    
    # Parámetros básicos adicionales
    'objective': 'binary:logistic',  # Tipo de problema
    'eval_metric': 'error',          # Métrica de evaluación
    'seed': 42,                      # Semilla aleatoria
    
    # Parámetros de regularización
    'reg_alpha': 0,                  # Regularización L1
    'reg_lambda': 1,                 # Regularización L2
}
num_round = 500
evallist = [(dtrain,'train'), (dwatch,'watchlist')]  # Necesario para el early stopping rounds

# Entrenando el algoritmo
xgBoost = XGBClassifier.train(param, dtrain, num_round, evallist,
                    early_stopping_rounds=5)




"""### Predicción del modelo"""

# Usando el modelo para predecir
X_train['probability'] = xgBoost.predict(XGBClassifier.DMatrix(X_train[features]))
X_test['probability'] = xgBoost.predict(XGBClassifier.DMatrix(X_test[features]))

X_train['prediction'] = X_train.probability.apply(lambda x:1 if x > 0.5 else 0)
X_test['prediction'] = X_test.probability.apply(lambda x:1 if x > 0.5 else 0)

"""# Performance"""

from sklearn.metrics import *

metricsXGB = pd.DataFrame({'metric':['AUC','Gini','Accuracy','Precision','Recall','F1-score'],
                                'xgb_train':[roc_auc_score(y_train, X_train.probability),
                                        (roc_auc_score(y_train, X_train.probability)*2-1),
                                        accuracy_score(y_train, X_train.prediction),
                                        precision_score(y_train, X_train.prediction),
                                        recall_score(y_train, X_train.prediction),
                                        f1_score(y_train, X_train.prediction)],

                                'xgb_test':[roc_auc_score(y_test, X_test.probability),
                                        (roc_auc_score(y_test, X_test.probability)*2-1),
                                        accuracy_score(y_test, X_test.prediction),
                                        precision_score(y_test, X_test.prediction),
                                        recall_score(y_test, X_test.prediction),
                                        f1_score(y_test, X_test.prediction)]})

# Specificity
tn, fp, fn, tp = confusion_matrix(y_test, X_test.prediction).ravel()

print(confusion_matrix(y_test, X_test.prediction))

accuracy = (tp + tn)/(tn + fp + tp + fn)
print('Test Specificity: ', accuracy)

