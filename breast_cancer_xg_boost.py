# -*- coding: utf-8 -*-
"""Breast Cancer XG-Boost

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/135aAWTG4llhQthH9xJ1xVqFEcCV94tGz

# 1. Cargando Dataset y librerías
"""

import sklearn
assert sklearn.__version__>='0.20'

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings(action='ignore', message='^internal gelsd')

df = pd.read_csv('data.csv')


# Validación
df.shape

# Vista general del dataset
df.info()

# Se observa: variables numéricas excepto el target(diagnosis)
# Variables como id y unamed no entran en el análisis

"""## Análisis del Target

"""

sns.countplot(x='diagnosis', data=df, palette ='hls')

# No se observa un desbalanceo de datos
# Debe pasarse a variable numérica para ser analizada

# Conteo de valores de la variable objetivo
df['diagnosis'].value_counts()

# B: Benigno, M: Maligno

# Realizando el cambio B->1, M->0
df['diagnosis'] = df['diagnosis'].replace({'M':0, 'B':1})

# Validando el cambio
df['diagnosis'].value_counts()

# Porcentaje de tumores benignos
df.diagnosis.mean()

# Identificación de variables
features = list(set(df.columns.tolist())-set(['Unnamed: 32', 'diagnosis', 'id']))
features

# Determinando matrices de datos
X = df[features]
y = df.diagnosis # Variable objetivo

"""# 2. Muestreo"""

# Muestreo de la data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify = y,
                                                    train_size = 0.6,
                                                    random_state = 123)

# Partiendo el 40% restante entre test y watch
X_watch, X_test, y_watch, y_test = train_test_split(X_test, y_test,
                                                    stratify = y_test,
                                                    train_size = 0.5,
                                                    random_state = 123)



"""## XgBoost"""

import numpy as np
import xgboost as XGBClassifier

# Para realizar el modelo de XGBoost necesitamos cambiar el formato de pd as DMatrix

# Cambiando formato a .DMatrix()
dtrain = XGBClassifier.DMatrix(X_train, label = y_train)
dwatch = XGBClassifier.DMatrix(X_watch, label = y_watch)
dtest = XGBClassifier.DMatrix(X_test, label = y_test)

# Definiendo parámetros
param ={'objective': 'binary:logistic', # Tipo de aprendizaje
          'max_depth': 2, # Profundidad del árbol
          'learning_rate': 0.15,
          'min_data_leaf': 0.05, # Min data en cada hoja
          'grow_policy': 'lossguide', #Aprendizaje por error
          'eval_metric': 'error' # Para evaluar el modelo
          #'seed': 123
          }
num_round = 500
evallist = [(dtrain,'train'), (dwatch,'watchlist')]

# Entrenando el algoritmo

# 
xgBoost = XGBClassifier.train(param, dtrain, num_round, evallist,
                    early_stopping_rounds=5)



"""### Variables del modelo

"""

# Importancia de las variables con .get_score()
# importance = xgBoost.get_score(importance_type = 'total_gain')
# importance

# total_gain: Sum tot de las ganancias de cada variable

# XGBClassifier.plot_importance(xgBoost, max_num_features=20, importance_type='total_gain')

# # Ordenando
# pdVarImp = pd.DataFrame({'Feature': list(importance.keys()),
#                          'Importance': list(importance.values())}).sort_values('Importance', ascending = False)

# pdVarImp['Orden'] = np.arange(len(pdVarImp)) + 1
# pdVarImp

# # Suma de los valores de ganancia
# plt.plot(pdVarImp.Orden, pdVarImp.Importance.cumsum(axis=0))

# # Reescalando
# pdVarImp['porc_gain'] = pdVarImp.Importance.apply(lambda x: x/pdVarImp.Importance.sum())
# plt.plot(pdVarImp.Orden, pdVarImp.porc_gain.cumsum(axis=0))

# pdVarImp['porc_gain_acum'] = pdVarImp.porc_gain.cumsum(axis = 0)



"""### Predicción del modelo"""

# Usando el modelo para predecir
X_train['probability'] = xgBoost.predict(XGBClassifier.DMatrix(X_train[features]))
X_test['probability'] = xgBoost.predict(XGBClassifier.DMatrix(X_test[features]))

X_train['prediction'] = X_train.probability.apply(lambda x:1 if x > 0.5 else 0)
X_test['prediction'] = X_test.probability.apply(lambda x:1 if x > 0.5 else 0)

"""# Performance"""

from sklearn.metrics import *

metricsXGB = pd.DataFrame({'metric':['AUC','Gini','Accuracy','Precision','Recall','F1-score'],
                                'xgb_train':[roc_auc_score(y_train, X_train.probability),
                                        (roc_auc_score(y_train, X_train.probability)*2-1),
                                        accuracy_score(y_train, X_train.prediction),
                                        precision_score(y_train, X_train.prediction),
                                        recall_score(y_train, X_train.prediction),
                                        f1_score(y_train, X_train.prediction)],

                                'xgb_test':[roc_auc_score(y_test, X_test.probability),
                                        (roc_auc_score(y_test, X_test.probability)*2-1),
                                        accuracy_score(y_test, X_test.prediction),
                                        precision_score(y_test, X_test.prediction),
                                        recall_score(y_test, X_test.prediction),
                                        f1_score(y_test, X_test.prediction)]})

print(metricsXGB)

# Specificity
tn, fp, fn, tp = confusion_matrix(y_test, X_test.prediction).ravel()

print(confusion_matrix(y_test, X_test.prediction))

specificity = tn/(tn+fp)
print('Test Specificity: ', specificity)

